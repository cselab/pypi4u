{
    "docs": [
        {
            "location": "/README/",
            "text": "PyPi4u\n\n\nPyPi4u is inteded to provide the user with easy-to-use uncertainty quantification tools written in Python. \nIt provides a covariance matrix adaptation evolution strategy implementation (CMA-ES) and a transitional markov-chain monte carlo (TMCMC) implementation to perform uncertainty quantification and parameter estimation. The CMA-ES implementation uses the covariance matrix adaptation evolution strategy to determine the maximum of the posterior probability distribution, which is defined as following: \n\n\n\n\nThe TMCMC algorithm avoids difficulties in sampling directly from the target posterior probability distribution by sampling from a series of intermediate probability distributions. This annealing process can be denoted by \n\n\n\n\nINSERT FORMULA\n\n\n\n\nThe generated samples can then be used to determine the stochastic mean and variance. The stochastic mean of the multivariate distribution can be equated to the most-likely parameters/estimators given the data. \n\n\nGetting started\n\n\nTo download the implementations, please visit the github \nrepository\n and clone it. \n\n\nHow it Works\n\n\nThe following section explains the project's underlying structure and how the provided code can be used to make estimations of the model parameters. This explanation is further supported by a proceeding example, which illustrates how the scripts can be implemented.\n\n\nCommon Parameters\n\n\nBoth the CMA-ES and TMCMC implementation access a common parameter file, named \ncommon_parameters.par\n. The common parameter file, which needs to be filled out by the user, defines the problem and therefore forms the project's foundation. The structure of the common parameter file is depicted below. It consists of three sections; the model, priors and log-likelihood. \n\n\n[MODEL]\nNumber of model parameters = 3\nmodel file = model_function.py\ndata file = data.txt \n\n[PRIORS]\n# Set prior distribution\n# prior distributions uniform normal\n\nP1 = normal 4 2\nP2 = normal 1 2\nP3 = uniform 0 5\nerror_prior = uniform 0 2\n\n[log-likelihood]\n# error either proportional or constant\nerror = constant\n\n\n\n\n[MODEL]\n - In the model section the number of model parameters is to be defined. The model parameters are the number of unknown parameters in the model function. In other words the model parameters are the parameters that are to be predicted. For example if the model function is the following: \n\n\n \n\n\nThe model parameters would be \n and thus the number of model parameters would be 3. The model file should be set equal to path of the python script that contains the function definition corresponding to the model function. Finally, the data file is the path to the text file that contains a list of input values and corresponding output values (function evaluations with noise).\n\n\n[PRIORS]\n - In this section the user is able to set the prior probability density functions of the estimators. The prior probability distribution functions can either be normal or uniform. They are assigned by writing to the parameter file P[number of parameter] = [normal] [mean] [variance] or P[number of parameter] = [uniform] [minimum] [maximum]. The error prior defines the prior knowledge available in regards to the noise that corrupts the data. Its definition is identical to that of the parameter priors, just that instead of P[number of parameter], the user must now set error_prior equal to a uniform or normal distribution.\n\n\n[log-likelihood]\n - In this section the error/noise that corrupts the data can be defined. A constant error means that the data is distorted by a constant term \n. In the case of a proportional error, the magnitude of the error also depends on \nt\n, the independent variable, as it is defined as \n, where \n. \n\n\nCMA Parameters\n\n\nBesides setting the common parameters, the user must also define parameters specific to the implementation. The CMA parameters, which are stored in \nCMA_parameters.par\n file, are the following: \n\n\n[PARAMETERS]\n#defining the parameters for CMA \n\nbounds = 0 10 #upper and lower bound, the parameters must be within these bounds \nx_0 = 5 5 5 5 #starting point, initial guess for the theta vector (the last entry of the vector corresponds to the guess of the error term)\nsigma_0 = 5 #initial standard deviation\n\n\n\n\nThese specific parameters can be interpreted as following:\n\n \nBounds\n - defines the lower and upper bound of the estimators. The values of all of the estimated parameters are restricted to this bound. The larger the bound the longer it will take for the CMA-ES algorithm to find the maximum of the posterior probability function. \n\n \nx_0\n - this is a vector containing the initial guesses of the estimators. The vector size exceeds the number of model parameters by one. The variance introduced by the noise (\n) is also an unknown that has to be predicted. It forms the last entry of theta vector. x_0 represents the starting point of the CMA-ES algorithm. Ultimately, the algorithm evolves from this guess towards the most-likely estimators. A rule of thumb is that the initial guesses should be in the middle of bound. If the lower bound is 0 and the upper bound is 10, the x_0 should be 5 5 5 5. \n* \nsigma_0\n - defines the initial standard deviation used by CMA-ES algorithm when making its initial guesses. \n\n\nTMCMC Parameters\n\n\nBesides the common parameters, also TMCMC requires additional parameters. They are included in the parameter file 'TMCMC.par' and are TMCMC specific parameters such as pop_size, bbeta = 0.04, tol_COV and BURN_IN. Further settings can be changed within the default settings folder.\n\n\nModel Function\n\n\nThe model function needs to be defined by the user. It is a function that takes two arguments, an estimator vector of a given size (size is defined in common parameters) and \nt\n, and returns a float. For example: \n\n\nimport math\n\ndef model_function(theta, time): #evaluates my model function for a given theta and time\n    return time*theta[2]*math.cos(theta[0]*time) + theta[1]*math.sin(time)\n\n\n\n\nData File\n\n\nThe user needs to append a data file. This data file should be a text file that contains two columns, delimited by a space. The first column should be the value of the independent variable [\nt\n], while the second column should be corresponding function evaluation/measurement [\nfunction evaluation\n]. \n\n\nExecuting the Code\n\n\nAfter having filled in the parameter files, the estimators for the model parameters are simply obtained by either running \nCMA_implementation.py\n or \nTMCMC_implementation.py\n. On execution a text file named \nCMA_estimators.txt\n or \nTMCMC_estimators.txt\n will be created, in which the values of the estimators are stored. The last estimator in the file corresponds to the error estimator. It estimates the variance of the noise, within the data set. \n\n\nExample Problem - DEMO\n\n\nGeneration of Synthetic Data\n\n\nSynthetic data was generated from a predefined model function:\n\n\n \n\n\nThe model parameters were set equal to \n. The function was then evaluated for \n. Additionally, random noise is introduced by simply adding epsilon to the function evaluations (constant error). The sum of the terms forms \n\n\n\n\nwhere epsilon equates to \n\n\nConsequently, all obtained function evaluations are independently and identically distributed, following a normal distribution with a variance of one. The synthetic data is stored in a text document \ndata.txt\n, which lists the input value \nt\n and the corresponding function value \nf\n. Both approaches use the synthetic data and the function definition \nf\n to approximate the values of the thetas and epsilon. \n\n\nCommon Parameters\n\n\n[MODEL]\nNumber of model parameters = 3\nmodel file = model_function.py\ndata file = data.txt \n\n[PRIORS]\n# Set prior distribution\n# prior distributions uniform normal\n\nP1 = normal 4 2\nP2 = normal 1 2\nP3 = uniform 0 5\nerror_prior = uniform 0 2\n\n[log-likelihood]\n# error either proportional or constant\nerror = constant\n\n\n\n\n[MODEL]\n - The model function consists of three parameters; therefore the number of model parameters was set to three. Additionally, the paths to the python model function and to the data file are given. \n\n\n[PRIORS]\n - In this exemplary case, the prior for the first parameter was taken to be a normal probability distribution with a mean of 4 and a variance of 2. The prior of the second parameter is also a normal probability distribution, but with a mean of 1 and a variance of 2. The third prior was set to a uniform probability distribution with a minimum of 0 and maximum of 5. Finally, the error prior was defined to be a uniform distribution with a minimum of 0 and maximum of 2. \n\n\n[log-likelihood]\n - The synthetic data was produced by corrupting the function evaluations with constant noise, which originated from a normal distribution with a mean of 0 and a variance of 1 (\n). Therefore, the error is set equal to a constant in the log-likelihood section of the common parameters. \n\n\nModel Function - Python Function\n\n\nThe model function is defined as following: \n\n\n \n\n\nTherefore, the first argument of the function, the theta vector, needs to be a vector of size three, as there are three model parameters. The resulting function definition is as following: \n\n\nimport math\n\ndef model_function(theta, time): #evaluates my model function for a given theta and time\n    return time*theta[2]*math.cos(theta[0]*time) + theta[1]*math.sin(time)\n\n\n\n\nBoth the CMA-ES and the TMCMC implementation call this python function.  \n\n\nCMA-ES Implementation\n\n\nTo be able to implement the CMA-ES algorithm the CMA parameters must still be defined.  \n\n\n[PARAMETERS]\n#defining the parameters for CMA \n\nbounds = 0 10 #upper and lower bound, the parameters must be within these bounds \nx_0 = 5 5 5 5 #starting point, initial guess for the theta vector (the last entry of the vector corresponds to the guess of the error term)\nsigma_0 = 5 #initial standard deviation\n\n\n\n\nIn this example all parameters lie within the bound [0,10]. Furthermore, the rule of thumb is applied to obtain an initial starting guess for the theta vector. Finally, the initial standard deviation of the CMA-ES alogrithm was defined to be 5.",
            "title": "Home"
        },
        {
            "location": "/README/#pypi4u",
            "text": "PyPi4u is inteded to provide the user with easy-to-use uncertainty quantification tools written in Python. \nIt provides a covariance matrix adaptation evolution strategy implementation (CMA-ES) and a transitional markov-chain monte carlo (TMCMC) implementation to perform uncertainty quantification and parameter estimation. The CMA-ES implementation uses the covariance matrix adaptation evolution strategy to determine the maximum of the posterior probability distribution, which is defined as following:    The TMCMC algorithm avoids difficulties in sampling directly from the target posterior probability distribution by sampling from a series of intermediate probability distributions. This annealing process can be denoted by    INSERT FORMULA   The generated samples can then be used to determine the stochastic mean and variance. The stochastic mean of the multivariate distribution can be equated to the most-likely parameters/estimators given the data.",
            "title": "PyPi4u"
        },
        {
            "location": "/README/#getting-started",
            "text": "To download the implementations, please visit the github  repository  and clone it.",
            "title": "Getting started"
        },
        {
            "location": "/README/#how-it-works",
            "text": "The following section explains the project's underlying structure and how the provided code can be used to make estimations of the model parameters. This explanation is further supported by a proceeding example, which illustrates how the scripts can be implemented.",
            "title": "How it Works"
        },
        {
            "location": "/README/#common-parameters",
            "text": "Both the CMA-ES and TMCMC implementation access a common parameter file, named  common_parameters.par . The common parameter file, which needs to be filled out by the user, defines the problem and therefore forms the project's foundation. The structure of the common parameter file is depicted below. It consists of three sections; the model, priors and log-likelihood.   [MODEL]\nNumber of model parameters = 3\nmodel file = model_function.py\ndata file = data.txt \n\n[PRIORS]\n# Set prior distribution\n# prior distributions uniform normal\n\nP1 = normal 4 2\nP2 = normal 1 2\nP3 = uniform 0 5\nerror_prior = uniform 0 2\n\n[log-likelihood]\n# error either proportional or constant\nerror = constant  [MODEL]  - In the model section the number of model parameters is to be defined. The model parameters are the number of unknown parameters in the model function. In other words the model parameters are the parameters that are to be predicted. For example if the model function is the following:      The model parameters would be   and thus the number of model parameters would be 3. The model file should be set equal to path of the python script that contains the function definition corresponding to the model function. Finally, the data file is the path to the text file that contains a list of input values and corresponding output values (function evaluations with noise).  [PRIORS]  - In this section the user is able to set the prior probability density functions of the estimators. The prior probability distribution functions can either be normal or uniform. They are assigned by writing to the parameter file P[number of parameter] = [normal] [mean] [variance] or P[number of parameter] = [uniform] [minimum] [maximum]. The error prior defines the prior knowledge available in regards to the noise that corrupts the data. Its definition is identical to that of the parameter priors, just that instead of P[number of parameter], the user must now set error_prior equal to a uniform or normal distribution.  [log-likelihood]  - In this section the error/noise that corrupts the data can be defined. A constant error means that the data is distorted by a constant term  . In the case of a proportional error, the magnitude of the error also depends on  t , the independent variable, as it is defined as  , where  .",
            "title": "Common Parameters"
        },
        {
            "location": "/README/#cma-parameters",
            "text": "Besides setting the common parameters, the user must also define parameters specific to the implementation. The CMA parameters, which are stored in  CMA_parameters.par  file, are the following:   [PARAMETERS]\n#defining the parameters for CMA \n\nbounds = 0 10 #upper and lower bound, the parameters must be within these bounds \nx_0 = 5 5 5 5 #starting point, initial guess for the theta vector (the last entry of the vector corresponds to the guess of the error term)\nsigma_0 = 5 #initial standard deviation  These specific parameters can be interpreted as following:   Bounds  - defines the lower and upper bound of the estimators. The values of all of the estimated parameters are restricted to this bound. The larger the bound the longer it will take for the CMA-ES algorithm to find the maximum of the posterior probability function.    x_0  - this is a vector containing the initial guesses of the estimators. The vector size exceeds the number of model parameters by one. The variance introduced by the noise ( ) is also an unknown that has to be predicted. It forms the last entry of theta vector. x_0 represents the starting point of the CMA-ES algorithm. Ultimately, the algorithm evolves from this guess towards the most-likely estimators. A rule of thumb is that the initial guesses should be in the middle of bound. If the lower bound is 0 and the upper bound is 10, the x_0 should be 5 5 5 5. \n*  sigma_0  - defines the initial standard deviation used by CMA-ES algorithm when making its initial guesses.",
            "title": "CMA Parameters"
        },
        {
            "location": "/README/#tmcmc-parameters",
            "text": "Besides the common parameters, also TMCMC requires additional parameters. They are included in the parameter file 'TMCMC.par' and are TMCMC specific parameters such as pop_size, bbeta = 0.04, tol_COV and BURN_IN. Further settings can be changed within the default settings folder.",
            "title": "TMCMC Parameters"
        },
        {
            "location": "/README/#model-function",
            "text": "The model function needs to be defined by the user. It is a function that takes two arguments, an estimator vector of a given size (size is defined in common parameters) and  t , and returns a float. For example:   import math\n\ndef model_function(theta, time): #evaluates my model function for a given theta and time\n    return time*theta[2]*math.cos(theta[0]*time) + theta[1]*math.sin(time)",
            "title": "Model Function"
        },
        {
            "location": "/README/#data-file",
            "text": "The user needs to append a data file. This data file should be a text file that contains two columns, delimited by a space. The first column should be the value of the independent variable [ t ], while the second column should be corresponding function evaluation/measurement [ function evaluation ].",
            "title": "Data File"
        },
        {
            "location": "/README/#executing-the-code",
            "text": "After having filled in the parameter files, the estimators for the model parameters are simply obtained by either running  CMA_implementation.py  or  TMCMC_implementation.py . On execution a text file named  CMA_estimators.txt  or  TMCMC_estimators.txt  will be created, in which the values of the estimators are stored. The last estimator in the file corresponds to the error estimator. It estimates the variance of the noise, within the data set.",
            "title": "Executing the Code"
        },
        {
            "location": "/README/#example-problem-demo",
            "text": "",
            "title": "Example Problem - DEMO"
        },
        {
            "location": "/README/#generation-of-synthetic-data",
            "text": "Synthetic data was generated from a predefined model function:     The model parameters were set equal to  . The function was then evaluated for  . Additionally, random noise is introduced by simply adding epsilon to the function evaluations (constant error). The sum of the terms forms    where epsilon equates to   Consequently, all obtained function evaluations are independently and identically distributed, following a normal distribution with a variance of one. The synthetic data is stored in a text document  data.txt , which lists the input value  t  and the corresponding function value  f . Both approaches use the synthetic data and the function definition  f  to approximate the values of the thetas and epsilon.",
            "title": "Generation of Synthetic Data"
        },
        {
            "location": "/README/#common-parameters_1",
            "text": "[MODEL]\nNumber of model parameters = 3\nmodel file = model_function.py\ndata file = data.txt \n\n[PRIORS]\n# Set prior distribution\n# prior distributions uniform normal\n\nP1 = normal 4 2\nP2 = normal 1 2\nP3 = uniform 0 5\nerror_prior = uniform 0 2\n\n[log-likelihood]\n# error either proportional or constant\nerror = constant  [MODEL]  - The model function consists of three parameters; therefore the number of model parameters was set to three. Additionally, the paths to the python model function and to the data file are given.   [PRIORS]  - In this exemplary case, the prior for the first parameter was taken to be a normal probability distribution with a mean of 4 and a variance of 2. The prior of the second parameter is also a normal probability distribution, but with a mean of 1 and a variance of 2. The third prior was set to a uniform probability distribution with a minimum of 0 and maximum of 5. Finally, the error prior was defined to be a uniform distribution with a minimum of 0 and maximum of 2.   [log-likelihood]  - The synthetic data was produced by corrupting the function evaluations with constant noise, which originated from a normal distribution with a mean of 0 and a variance of 1 ( ). Therefore, the error is set equal to a constant in the log-likelihood section of the common parameters.",
            "title": "Common Parameters"
        },
        {
            "location": "/README/#model-function-python-function",
            "text": "The model function is defined as following:      Therefore, the first argument of the function, the theta vector, needs to be a vector of size three, as there are three model parameters. The resulting function definition is as following:   import math\n\ndef model_function(theta, time): #evaluates my model function for a given theta and time\n    return time*theta[2]*math.cos(theta[0]*time) + theta[1]*math.sin(time)  Both the CMA-ES and the TMCMC implementation call this python function.",
            "title": "Model Function - Python Function"
        },
        {
            "location": "/README/#cma-es-implementation",
            "text": "To be able to implement the CMA-ES algorithm the CMA parameters must still be defined.    [PARAMETERS]\n#defining the parameters for CMA \n\nbounds = 0 10 #upper and lower bound, the parameters must be within these bounds \nx_0 = 5 5 5 5 #starting point, initial guess for the theta vector (the last entry of the vector corresponds to the guess of the error term)\nsigma_0 = 5 #initial standard deviation  In this example all parameters lie within the bound [0,10]. Furthermore, the rule of thumb is applied to obtain an initial starting guess for the theta vector. Finally, the initial standard deviation of the CMA-ES alogrithm was defined to be 5.",
            "title": "CMA-ES Implementation"
        },
        {
            "location": "/getting_started/",
            "text": "Getting started\n\n\nBoth algorithms are implemented in Python 3 and require the installation of additional packages.\nTo download the current code, please visit the github \nrepository\n and clone it. \n\n\nInstallation\n\n\nBoth implementations rely on mostly well-known packages as, e.g. numpy. To use the code, please install the required packages via \"pip3 install \npackage\n\". \n\n\nCMA-ES\n requires the packages\n\n\n\n\ncma 2.5.3 - https://pypi.python.org/pypi/cma,\n\n\nnumpy,\n\n\nConfigParser,\n\n\nand matplotlib.\n\n\n\n\nTMCMC\n requires the packages\n\n\n\n\nnumpy,\n\n\nscipy, \n\n\nmatplotlib,\n\n\nand configparser.\n\n\n\n\nRunning an Example\n\n\nPyPi4u comes with a ready-to-run example including a model, synthetic data and predefined parameters. \n\n\nModel and Synthetic Data\n\n\nThe model is given by the equation \n\n\n.\n\n\nThe model parameters were set equal to \n and the function was evaluated for \n. Random noise is introduced by simply adding epsilon to the function evaluations (constant error). The sum of the terms forms \n\n\n\n\nwhere epsilon equates to \n\n\nConsequently, all obtained function evaluations are independently and identically distributed, following a normal distribution with a variance of one. The synthetic data is stored in a text document \ndata.txt\n, which lists the input value \nt\n and the corresponding function value \nf\n. Both approaches use the synthetic data and the function definition \nf\n to approximate the values of the thetas and epsilon. \n\n\nPlay with Parameters\n\n\nThe parameters for the CMA-ES and TMCMC algorithms can be set in the common_parameters.par, CMA.par and the TMCMC.par files. A more detailed description of the parameters and how to set them, can be found in the following. For the example, they are already set in a feasable fashion.\n\n\nHow to run the Example\n\n\nRun \nCMA_implementation.py\n or \nsequential_tmcmc.py\n to execute the CMA implementation or TMCMC implementation, respectively. On execution of the CMA implementation a text file named \nCMA_estimators.txt\n is created, in which the values of the estimators are stored. For the TMCMC implementation, \"curgen_db_***.dat\" files are generated, corresponding to the different generations. The results can be plotted by \"plotting.py\".",
            "title": "Getting started"
        },
        {
            "location": "/getting_started/#getting-started",
            "text": "Both algorithms are implemented in Python 3 and require the installation of additional packages.\nTo download the current code, please visit the github  repository  and clone it.",
            "title": "Getting started"
        },
        {
            "location": "/getting_started/#installation",
            "text": "Both implementations rely on mostly well-known packages as, e.g. numpy. To use the code, please install the required packages via \"pip3 install  package \".   CMA-ES  requires the packages   cma 2.5.3 - https://pypi.python.org/pypi/cma,  numpy,  ConfigParser,  and matplotlib.   TMCMC  requires the packages   numpy,  scipy,   matplotlib,  and configparser.",
            "title": "Installation"
        },
        {
            "location": "/getting_started/#running-an-example",
            "text": "PyPi4u comes with a ready-to-run example including a model, synthetic data and predefined parameters.",
            "title": "Running an Example"
        },
        {
            "location": "/getting_started/#model-and-synthetic-data",
            "text": "The model is given by the equation   .  The model parameters were set equal to   and the function was evaluated for  . Random noise is introduced by simply adding epsilon to the function evaluations (constant error). The sum of the terms forms    where epsilon equates to   Consequently, all obtained function evaluations are independently and identically distributed, following a normal distribution with a variance of one. The synthetic data is stored in a text document  data.txt , which lists the input value  t  and the corresponding function value  f . Both approaches use the synthetic data and the function definition  f  to approximate the values of the thetas and epsilon.",
            "title": "Model and Synthetic Data"
        },
        {
            "location": "/getting_started/#play-with-parameters",
            "text": "The parameters for the CMA-ES and TMCMC algorithms can be set in the common_parameters.par, CMA.par and the TMCMC.par files. A more detailed description of the parameters and how to set them, can be found in the following. For the example, they are already set in a feasable fashion.",
            "title": "Play with Parameters"
        },
        {
            "location": "/getting_started/#how-to-run-the-example",
            "text": "Run  CMA_implementation.py  or  sequential_tmcmc.py  to execute the CMA implementation or TMCMC implementation, respectively. On execution of the CMA implementation a text file named  CMA_estimators.txt  is created, in which the values of the estimators are stored. For the TMCMC implementation, \"curgen_db_***.dat\" files are generated, corresponding to the different generations. The results can be plotted by \"plotting.py\".",
            "title": "How to run the Example"
        },
        {
            "location": "/own_model/",
            "text": "Set up own model\n\n\nModel Function\n\n\nThe example model function is defined as following: \n\n\n \n\n\nTherefore, the first argument of the function, the theta vector, needs to be a vector of size three, as there are three model parameters. The resulting function definition is as following: \n\n\nimport math\n\ndef model_function(theta, time): #evaluates my model function for a given theta and time\n    return time*theta[2]*math.cos(theta[0]*time) + theta[1]*math.sin(time)\n\n\n\n\nBoth the CMA-ES and the TMCMC implementation call this python function.  \n\n\nParameters\n\n\nBoth CMA-ES and TMCMC enable the user to tweak the algorithms by changing some parameters. We split the parameters in a common_parameters.par, CMA_parameters.par and TMCMC.par file, with the common, the CMA specific and TMCMC specific parameters, respectively. \n\n\nCommon Parameters\n\n\n[MODEL]\nNumber of model parameters = 3\nmodel file = model_function.py\ndata file = data.txt \n\n[PRIORS]\n# Set prior distribution\n# prior distributions uniform normal\n\nP1 = normal 4 2\nP2 = normal 1 2\nP3 = uniform 0 5\nerror_prior = uniform 0 2\n\n[log-likelihood]\n# error either proportional or constant\nerror = constant\n\n\n\n\n[MODEL]\n - The model function comprises three parameters; therefore the number of model parameters was set to three. Additionally, the relative paths to the python model function and to the data file are given. \n\n\n[PRIORS]\n - In priors you can encode your prior beliefs about the parameters. Follow the following formats:\n\n\n\n\nnormal distributed parameter i: Pi = normal mean variance\n\n\nuniformly distributed parameter i: Pi = normal lower_bound upper_bound\n\n\n\n\nIn this exemplary case, the prior for the first parameter was taken to be a normal probability distribution with a mean of 4 and a variance of 2. The prior of the second parameter is also a normal probability distribution, but with a mean of 1 and a variance of 2. The third prior was set to a uniform probability distribution with a minimum of 0 and maximum of 5. Finally, the error prior was defined to be a uniform distribution with a minimum of 0 and maximum of 2. \n\n\n[log-likelihood]\n - The synthetic data was produced by corrupting the function evaluations with constant noise, which originated from a normal distribution with a mean of 0 and a variance of 1 (\n). Therefore, the error is set equal to a constant in the log-likelihood section of the common parameters. \n\n\nCMA-ES Implementation\n\n\nTo be able to implement the CMA-ES algorithm the CMA parameters must still be defined.  \n\n\n[PARAMETERS]\n#defining the parameters for CMA \n\nbounds = 0 10 #upper and lower bound, the parameters must be within these bounds \nx_0 = 5 5 5 5 #starting point, initial guess for the theta vector (the last entry of the vector corresponds to the guess of the error term)\nsigma_0 = 5 #initial standard deviation\n\n\n\n\nIn this example all parameters lie within the bound [0,10]. Furthermore, the rule of thumb is applied to obtain an initial starting guess for the theta vector. Finally, the initial standard deviation of the CMA-ES alogrithm was defined to be 5.",
            "title": "How to implement your own model"
        },
        {
            "location": "/own_model/#set-up-own-model",
            "text": "",
            "title": "Set up own model"
        },
        {
            "location": "/own_model/#model-function",
            "text": "The example model function is defined as following:      Therefore, the first argument of the function, the theta vector, needs to be a vector of size three, as there are three model parameters. The resulting function definition is as following:   import math\n\ndef model_function(theta, time): #evaluates my model function for a given theta and time\n    return time*theta[2]*math.cos(theta[0]*time) + theta[1]*math.sin(time)  Both the CMA-ES and the TMCMC implementation call this python function.",
            "title": "Model Function"
        },
        {
            "location": "/own_model/#parameters",
            "text": "Both CMA-ES and TMCMC enable the user to tweak the algorithms by changing some parameters. We split the parameters in a common_parameters.par, CMA_parameters.par and TMCMC.par file, with the common, the CMA specific and TMCMC specific parameters, respectively.",
            "title": "Parameters"
        },
        {
            "location": "/own_model/#common-parameters",
            "text": "[MODEL]\nNumber of model parameters = 3\nmodel file = model_function.py\ndata file = data.txt \n\n[PRIORS]\n# Set prior distribution\n# prior distributions uniform normal\n\nP1 = normal 4 2\nP2 = normal 1 2\nP3 = uniform 0 5\nerror_prior = uniform 0 2\n\n[log-likelihood]\n# error either proportional or constant\nerror = constant  [MODEL]  - The model function comprises three parameters; therefore the number of model parameters was set to three. Additionally, the relative paths to the python model function and to the data file are given.   [PRIORS]  - In priors you can encode your prior beliefs about the parameters. Follow the following formats:   normal distributed parameter i: Pi = normal mean variance  uniformly distributed parameter i: Pi = normal lower_bound upper_bound   In this exemplary case, the prior for the first parameter was taken to be a normal probability distribution with a mean of 4 and a variance of 2. The prior of the second parameter is also a normal probability distribution, but with a mean of 1 and a variance of 2. The third prior was set to a uniform probability distribution with a minimum of 0 and maximum of 5. Finally, the error prior was defined to be a uniform distribution with a minimum of 0 and maximum of 2.   [log-likelihood]  - The synthetic data was produced by corrupting the function evaluations with constant noise, which originated from a normal distribution with a mean of 0 and a variance of 1 ( ). Therefore, the error is set equal to a constant in the log-likelihood section of the common parameters.",
            "title": "Common Parameters"
        },
        {
            "location": "/own_model/#cma-es-implementation",
            "text": "To be able to implement the CMA-ES algorithm the CMA parameters must still be defined.    [PARAMETERS]\n#defining the parameters for CMA \n\nbounds = 0 10 #upper and lower bound, the parameters must be within these bounds \nx_0 = 5 5 5 5 #starting point, initial guess for the theta vector (the last entry of the vector corresponds to the guess of the error term)\nsigma_0 = 5 #initial standard deviation  In this example all parameters lie within the bound [0,10]. Furthermore, the rule of thumb is applied to obtain an initial starting guess for the theta vector. Finally, the initial standard deviation of the CMA-ES alogrithm was defined to be 5.",
            "title": "CMA-ES Implementation"
        }
    ]
}